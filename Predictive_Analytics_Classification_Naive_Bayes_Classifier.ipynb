{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JNcymC1m6ka"
      },
      "source": [
        "# Naive Bayes Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDvU_4AtnBeN"
      },
      "source": [
        "* Naive Bayes algorithm is a supervised learning algorithm, which is based on **Bayes theorem** and used for solving classification problems.\n",
        "* It is mainly used in **text classification** that includes a high-dimensional training dataset.\n",
        "* Naive Bayes Classifier is one of the simple and most effective Classification algorithms which helps in building the fast machine learning models that can make **quick predictions**.\n",
        "* It is a **probabilistic classifier**, which means it predicts on the basis of the probability of an object.\n",
        "* Some popular examples of Naïve Bayes Algorithm are **spam filtration, Sentimental analysis, and classifying articles**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkiiMwXdT-sO"
      },
      "source": [
        "## Bayes’ Theorem\n",
        "\n",
        "Bayes’ Theorem finds the probability of an event occurring given the probability of another event that has already occurred. Bayes’ theorem is stated mathematically as the following equation:\n",
        "\n",
        "$$P(A|B) = \\frac{P(A)P(B|A)}{P(B)} $$\n",
        "\n",
        "where $A$ and $B$ are events and $P(B) \\neq 0$\n",
        "\n",
        "* Basically, we are trying to find probability of event $A$, given the event $B$ is true. Event $B$ is also termed as **evidence**.\n",
        "* $P(A)$ is the priori of $A$ (the **prior probability**, i.e. Probability of event before evidence is seen). The evidence is an attribute value of an unknown instance(here, it is event $B$).\n",
        "* $P(B)$ is **Marginal Probability**: Probability of Evidence.\n",
        "* $P(A|B)$ is a **posteriori probability** of $B$, i.e. probability of event after evidence is seen.\n",
        "$P(B|A)$ is Likelihood probability i.e the likelihood that a hypothesis will come true based on the evidence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmjMUFv2YmE7"
      },
      "source": [
        "Now, with regards to our dataset, we can apply Bayes’ theorem in following way:\n",
        "\n",
        "$$P(y|X) = \\frac{P(y)P(X|y)}{P(X)} $$\n",
        "\n",
        "where, $y$ is class variable and $X$ is a dependent feature vector where:\n",
        "$X=(x_1,x_2,x_3,\\dots,x_n)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3v2lC0TpZdCS"
      },
      "source": [
        "## Naive assumption\n",
        "\n",
        "Now, its time to put a naive assumption to the Bayes’ theorem, which is, independence among the features. So now, we split evidence into the independent parts.\n",
        "\n",
        "Now, if any two events A and B are independent, then,\n",
        "\n",
        "$P(A,B) = P(A)P(B)$ OR $P(A\\cap B) = P(A)P(B)$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upx5FT8BdnpM"
      },
      "source": [
        "## Model\n",
        "\n",
        "Hence, we reach to the result:\n",
        "$$\n",
        "P\\left(y \\mid x_1, \\ldots, x_n\\right)=\\frac{P\\left(x_1 \\mid y\\right) P\\left(x_1 \\mid y\\right) \\ldots P\\left(x_n \\mid y\\right) P(y)}{P\\left(x_1\\right) P\\left(x_2\\right) \\ldots P\\left(x_n\\right)}\n",
        "$$\n",
        "which can be expressed as:\n",
        "$$\n",
        "P\\left(y \\mid x_1, \\ldots, x_n\\right)=\\frac{P(y) \\prod_{i=1}^n P\\left(x_i \\mid y\\right)}{P\\left(x_1\\right) P\\left(x_2\\right) \\ldots P\\left(x_n\\right)}\n",
        "$$\n",
        "\n",
        "Now, as the denominator remains constant for a given input, we can remove that term:\n",
        "$$\n",
        "P\\left(y \\mid x_1, \\ldots, x_n\\right) \\propto P(y) \\prod_{i=1}^n P\\left(x_i \\mid y\\right)\n",
        "$$\n",
        "\n",
        "Now, we need to create a classifier model. For this, we find the probability of given set of inputs for all possible values of the class variable $y$ and pick up the output with maximum probability. This can be expressed mathematically as:\n",
        "$$\n",
        "y=\\operatorname{argmax}_y P(y) \\prod_{i=1}^n P\\left(x_i \\mid y\\right)\n",
        "$$\n",
        "\n",
        "So, finally, we are left with the task of calculating $P(y)$ and $P(x_i|y)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VcqBk8sdf2A"
      },
      "source": [
        "## Example: Spam Email Detection\n",
        "\n",
        "* Dataset: spam.csv\n",
        "* This is a csv file containing related information of 5172 randomly picked email files and their respective labels for spam or not-spam classification.\n",
        "* Source: https://www.kaggle.com/code/satarupadeb/na-ve-bayes-classification-spam-email-detection/input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-N1SGJF3jDGb"
      },
      "source": [
        "### Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6-D9F_rdjGuL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Reading the data from .csv file\n",
        "data = pd.read_csv('spam.csv', encoding='latin-1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "3ZAwNPp_2-wT",
        "outputId": "c2ec28cc-aad6-4c32-d5f8-bc6ed734e84b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Label                                               Text\n",
              "0   ham  Go until jurong point, crazy.. Available only ...\n",
              "1   ham                      Ok lar... Joking wif u oni...\n",
              "2  spam  Free entry in 2 a wkly comp to win FA Cup fina..."
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# display the first 3 rows\n",
        "data.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHP381BoBY35"
      },
      "source": [
        "### Train Test Split\n",
        "Let us split the dataset into train and test datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "YLWUFtjkBbvE"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Separate features (X) and target labels (y)\n",
        "X =  data['Text']\n",
        "y = data['Label']\n",
        "\n",
        "# Split the data into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nITNzZ_O6E52"
      },
      "source": [
        "**Question**: Define the target variable and the features from the dataset. Is it possible to consider the column 'Text' as the feature vector?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KF8UdcLz5alj"
      },
      "source": [
        "### Feature Extraction\n",
        "* Feature extraction refers to the process of transforming raw data into numerical features that can be processed while preserving the information in the original data set.\n",
        "* The **sklearn.feature_extraction** module can be used to extract features in a format supported by machine learning algorithms from datasets consisting of formats such as text and image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajcOh3z98aXB"
      },
      "source": [
        "#### Text feature extraction\n",
        "\n",
        "* We call **vectorization** the general process of turning a collection of text documents into numerical feature vectors.\n",
        "* the main text vectorizers provided by `scikit-learn`:\n",
        "> 1. **CountVectorizer:** Converts a collection of text documents to a matrix of token counts.\n",
        "> 2. **TfidfVectorizer:** Converts a collection of raw documents to a matrix of TF-IDF features.\n",
        "> 3. **HashingVectorizer:** Converts a collection of text documents to a matrix of token occurrences using a hashing trick.\n",
        "> 4. **DictVectorizer:** Transforms lists of feature-value mappings to vectors.\n",
        "\n",
        "If you what to know more about these please refer to:\n",
        "\n",
        "https://scikit-learn.org/stable/modules/feature_extraction.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "LkmhpDTR_lQQ"
      },
      "outputs": [],
      "source": [
        "# Example of Count vectorization for feature extraction\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Create a CountVectorizer instance\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit and transform the training data (X_train)\n",
        "X_train_vectorized = vectorizer.fit_transform(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1978     ham\n",
            "3989    spam\n",
            "3935     ham\n",
            "4078     ham\n",
            "4086    spam\n",
            "        ... \n",
            "3772     ham\n",
            "5191     ham\n",
            "5226     ham\n",
            "5390     ham\n",
            "860      ham\n",
            "Name: Label, Length: 4457, dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WBu4kWDgrAw"
      },
      "source": [
        "### Different Naive Bayes algorithms in sklearn\n",
        "\n",
        "\n",
        "*  **BernoulliNB**: The Bernoulli model is suitable when our feature vectors are binary, meaning they can only take two values (usually 0 and 1). In the context of text classification with a 'bag of words' model, the 1s represent \"word occurs in the document,\" and the 0s represent \"word does not occur in the document.\" This model is useful when we want to represent presence or absence of certain features in our data.\n",
        "\n",
        "* **CategoricalNB**: Naive Bayes classifier for categorical features.\n",
        "\n",
        "* **ComplementNB**: The Complement Naive Bayes classifier described in Rennie et al. (2003).\n",
        "\n",
        "* **GaussianNB**: In classification, Gaussian is a method that assumes the features we use to describe data (like measurements or characteristics) follow a normal distribution. This means that most of the data points cluster around the average value, and fewer data points deviate far from this average.\n",
        "\n",
        "* **MultinomialNB**: Multinomial is used when we are dealing with discrete counts. For example, in text classification, instead of just checking if a word occurs in a document (like in Bernoulli), we now count how many times a word appears in the document. It's like counting how many times a specific outcome (word) is observed over several trials (words in the document).\n",
        "\n",
        "If you what to know more about these please refer to:\n",
        "1. https://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "9IGhGfoMjgFX",
        "outputId": "a4f6c1ff-6475-4f5c-a39b-4e95301e96f2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BernoulliNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BernoulliNB</label><div class=\"sk-toggleable__content\"><pre>BernoulliNB()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "BernoulliNB()"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Multinomial Naive Bayes\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "\n",
        "# Train the Multinomial Naive Bayes classifier\n",
        "classifier = BernoulliNB()\n",
        "classifier.fit(X_train_vectorized, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkZhziEGE5q6"
      },
      "source": [
        "### Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "MultinomialNB()"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# # Multinomial Naive Bayes\n",
        "# from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# # Train the Multinomial Naive Bayes classifier\n",
        "# classifier = MultinomialNB()\n",
        "# classifier.fit(X_train_vectorized, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TeyaAM3FegS"
      },
      "source": [
        "* To predict whether a new email is spam or ham, we first need to vectorize the new email using the same vectorization technique that we applied to the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "j6XFQMo2E5CY"
      },
      "outputs": [],
      "source": [
        "# Transform the test data (X_test)\n",
        "X_test_vectorized = vectorizer.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "swIi1YYoGHPX"
      },
      "outputs": [],
      "source": [
        "# Make predictions on the test data\n",
        "y_pred = classifier.predict(X_test_vectorized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.97      1.00      0.99       965\n",
            "        spam       1.00      0.81      0.90       150\n",
            "\n",
            "    accuracy                           0.97      1115\n",
            "   macro avg       0.99      0.91      0.94      1115\n",
            "weighted avg       0.98      0.97      0.97      1115\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_rep)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEIV6nG5HXTq"
      },
      "source": [
        "### Model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRONv1YpHK1x",
        "outputId": "12c17be8-56a2-4abc-d38a-52d3d63011cd"
      },
      "outputs": [],
      "source": [
        "# from sklearn.metrics import classification_report\n",
        "\n",
        "# classification_rep = classification_report(y_test, y_pred)\n",
        "\n",
        "# print(\"Classification Report:\")\n",
        "# print(classification_rep)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCVRt6uvISsa"
      },
      "source": [
        "# Pipeline\n",
        "A sequence of data transformers with an optional final predictor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qsd5ZfRIw-I"
      },
      "source": [
        "* `Pipeline` allows you to sequentially apply a list of transformers to preprocess the data and, if desired, conclude the sequence with a final predictor for predictive modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9chP-MEJOj6",
        "outputId": "c1aca9f6-baf0-433a-8ad1-e1c9e4eedc39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.98      1.00      0.99       965\n",
            "        spam       0.99      0.89      0.94       150\n",
            "\n",
            "    accuracy                           0.98      1115\n",
            "   macro avg       0.98      0.95      0.96      1115\n",
            "weighted avg       0.98      0.98      0.98      1115\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Reading the data from .csv file\n",
        "data = pd.read_csv('spam.csv', encoding='latin-1')\n",
        "\n",
        "# Split the data into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['Text'], data['Label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# pipeline of CountVectorizer followed by MultinomialNB Classifier\n",
        "spam_filter = Pipeline([\n",
        "    ('vectorizer', CountVectorizer()),\n",
        "    ('classifier', MultinomialNB())\n",
        "])\n",
        "\n",
        "# train on count vectors with Naive Bayes\n",
        "spam_filter.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = spam_filter.predict(X_test)\n",
        "\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_rep)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygBJJXEnPOWT"
      },
      "source": [
        "# Laplace smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqvzRW-5PyU6"
      },
      "source": [
        "Laplace smoothing, also known as additive smoothing, is a technique used in Naive Bayes (NB) classifiers to handle the problem of zero probability in categorical data. It's particularly useful when dealing with text classification where the dataset may not be comprehensive enough to include all possible words in the training sample.\n",
        "\n",
        "Here's how it works:\n",
        "- **Zero Probability Issue**: Without smoothing, if a given word or feature does not occur in the training data, the probability estimate for that word given a class would be zero. This would make the posterior probability of the class zero, which can skew the results of the classifier.\n",
        "- **Additive Smoothing**: To avoid this, Laplace smoothing adds a small positive value (usually 1) to the count of each word for every class, regardless of whether it has been observed in the training data.\n",
        "- **Updated Probabilities**: The probability estimates are then adjusted accordingly. This ensures that no probability is exactly zero, and the classifier can handle unseen words in the test data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Roegv4WtQI8M"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "class sklearn.naive_bayes.MultinomialNB(*, alpha=1.0, force_alpha=True, fit_prior=True, class_prior=None)\n",
        "```\n",
        "**Parameters:**\n",
        "\n",
        "**alpha**: float or array-like of shape (n_features,), default=1.0\n",
        "\n",
        "Additive (Laplace/Lidstone) smoothing parameter (set alpha=0 and force_alpha=True, for no smoothing).\n",
        "\n",
        "**force_alpha**: bool, default=True\n",
        "\n",
        "If False and alpha is less than 1e-10, it will set alpha to 1e-10. If True, alpha will remain unchanged. This may cause numerical errors if alpha is too close to 0.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eO2psXpHQtSx"
      },
      "source": [
        "1. https://www.kaggle.com/code/satarupadeb/na-ve-bayes-classification-spam-email-detection/notebook\n",
        "2. https://www.geeksforgeeks.org/naive-bayes-classifiers/\n",
        "3. https://www.javatpoint.com/machine-learning-naive-bayes-classifier\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
